{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "full_df = pd.read_csv(\"/home/jiwon/jiwon/pygbased/data/ML1M/raw/u.data\", sep = '\\t', names = ['user', 'item', 'rating', 'time'])\n",
    "full_df.sort_values(by = 'time', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "full_df.user = full_df.user - 1\n",
    "full_df.item = full_df.item - 1\n",
    "print(f\"\"\"{full_df.item.unique().min()}\n",
    "{full_df.user.unique().min()}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_core(df, user_col='user', item_col='item', k=20):\n",
    "    \"\"\"\n",
    "    Filter the DataFrame to keep only interactions where each user and each item\n",
    "    has at least 20 interactions.\n",
    "    \"\"\"\n",
    "    # Repeat until the minimum interaction threshold is met for all users and items\n",
    "    while True:\n",
    "        # Count the number of interactions for each user and item\n",
    "        user_interactions = df[user_col].value_counts()\n",
    "        item_interactions = df[item_col].value_counts()\n",
    "        \n",
    "        # Identify users and items with fewer than 20 interactions\n",
    "        few_user_interactions = user_interactions[user_interactions < k].index.tolist()\n",
    "        few_item_interactions = item_interactions[item_interactions < k].index.tolist()\n",
    "        \n",
    "        # If all users and items have at least 20 interactions, stop\n",
    "        if len(few_user_interactions) == 0 and len(few_item_interactions) == 0:\n",
    "            break\n",
    "        \n",
    "        # Otherwise, remove users and items with too few interactions\n",
    "        df = df[~df[user_col].isin(few_user_interactions)]\n",
    "        df = df[~df[item_col].isin(few_item_interactions)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "filtered = filter_core(full_df, k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user:943\n",
      "item:1152\n",
      "interaction:97953\n",
      "density:0.09016823524213503\n",
      "time start:874724710\n",
      "time end:893286638\n"
     ]
    }
   ],
   "source": [
    "def print_static(df):\n",
    "    n_u = df['user'].nunique()\n",
    "    n_i = df['item'].nunique()\n",
    "    n_interaction = df.shape[0]\n",
    "    avg_u_d = n_interaction/n_u\n",
    "    density = n_interaction / (n_u*n_i)\n",
    "    time_start = df['time'].iloc[0]\n",
    "    time_end = df['time'].iloc[-1]\n",
    "    \n",
    "    print(f\"user:{n_u}\")\n",
    "    print(f\"item:{n_i}\")\n",
    "    print(f\"interaction:{n_interaction}\")\n",
    "    print(f\"density:{density}\")\n",
    "    print(f\"time start:{time_start}\")\n",
    "    print(f\"time end:{time_end}\")\n",
    "\n",
    "print_static(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "def split_df(df, base_size ,inc_step):\n",
    "    ratio = [base_size]\n",
    "    ratio += [(1-base_size)/inc_step]*inc_step\n",
    "    total_len = len(df)\n",
    "    start=0\n",
    "    result=[]\n",
    "    \n",
    "    for r in ratio:\n",
    "        end=start+floor(total_len*r)\n",
    "        result.append(df.iloc[start:end].reset_index(drop=True))\n",
    "        start=end\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[       user  item  rating       time\n",
       " 0       258   254       4  874724710\n",
       " 1       258   285       4  874724727\n",
       " 2       258   297       4  874724754\n",
       " 3       258   184       4  874724781\n",
       " 4       258   172       4  874724843\n",
       " ...     ...   ...     ...        ...\n",
       " 48971   767  1013       2  882816126\n",
       " 48972   828   249       3  882816754\n",
       " 48973   828   221       4  882816987\n",
       " 48974   667   895       4  882818549\n",
       " 48975   667   287       4  882818604\n",
       " \n",
       " [48976 rows x 4 columns],\n",
       "       user  item  rating       time\n",
       " 0      454   297       4  882818787\n",
       " 1       84   418       5  882819427\n",
       " 2       84   160       4  882819528\n",
       " 3       28   301       4  882820663\n",
       " 4       28   285       5  882820663\n",
       " ...    ...   ...     ...        ...\n",
       " 9790   523   572       4  884636827\n",
       " 9791   523   435       4  884636864\n",
       " 9792   523   229       3  884636907\n",
       " 9793   523   525       3  884636907\n",
       " 9794   523   548       4  884636931\n",
       " \n",
       " [9795 rows x 4 columns],\n",
       "       user  item  rating       time\n",
       " 0      523  1043       4  884636931\n",
       " 1      523   795       3  884636958\n",
       " 2      523    71       4  884636958\n",
       " 3      523   549       3  884636958\n",
       " 4      523   941       4  884636980\n",
       " ...    ...   ...     ...        ...\n",
       " 9790   109   324       3  886987561\n",
       " 9791   109   688       3  886987584\n",
       " 9792   109   687       1  886987605\n",
       " 9793   109   314       4  886987726\n",
       " 9794   109    21       4  886987826\n",
       " \n",
       " [9795 rows x 4 columns],\n",
       "       user  item  rating       time\n",
       " 0      109    11       4  886987826\n",
       " 1      109    68       4  886987860\n",
       " 2      109   214       3  886987894\n",
       " 3      109    63       4  886987894\n",
       " 4      109    10       4  886987922\n",
       " ...    ...   ...     ...        ...\n",
       " 9790   238     8       5  889180446\n",
       " 9791   238   432       5  889180447\n",
       " 9792   238   220       5  889180447\n",
       " 9793   238    90       4  889180487\n",
       " 9794   238   633       4  889180487\n",
       " \n",
       " [9795 rows x 4 columns],\n",
       "       user  item  rating       time\n",
       " 0      238  1097       5  889180487\n",
       " 1      238   515       5  889180487\n",
       " 2      238    45       4  889180487\n",
       " 3      238   649       5  889180530\n",
       " 4      238   496       4  889180578\n",
       " ...    ...   ...     ...        ...\n",
       " 9790    99   322       3  891375359\n",
       " 9791    99   288       3  891375359\n",
       " 9792    99   677       3  891375428\n",
       " 9793    99   989       3  891375428\n",
       " 9794    99   897       4  891375454\n",
       " \n",
       " [9795 rows x 4 columns],\n",
       "       user  item  rating       time\n",
       " 0       99   341       3  891375454\n",
       " 1       99   891       2  891375484\n",
       " 2       99   265       2  891375484\n",
       " 3       99   885       3  891375522\n",
       " 4       99   309       3  891375522\n",
       " ...    ...   ...     ...        ...\n",
       " 9790   728   332       4  893286638\n",
       " 9791   728   327       3  893286638\n",
       " 9792   728   747       4  893286638\n",
       " 9793   728   271       4  893286638\n",
       " 9794   728   299       4  893286638\n",
       " \n",
       " [9795 rows x 4 columns]]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_ml = split_df(filtered,0.5,5)\n",
    "splitted_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class DataInformation(object):\n",
    "    def __init__(self, total_df, user_id_mapping, item_id_mapping, id_user_mapping, id_item_mapping, user_consumed, item_consumed, n_user, n_item, user_unique_value,item_unique_value, seed):\n",
    "        self.total_df = total_df\n",
    "        self.user_id_mapping = user_id_mapping\n",
    "        self.item_id_mapping = item_id_mapping\n",
    "\n",
    "        self.id_user_mapping = id_user_mapping\n",
    "        self.id_item_mapping = id_item_mapping\n",
    "\n",
    "        self.user_consumed = user_consumed\n",
    "        self.item_consumed = item_consumed\n",
    "        \n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "\n",
    "        self.user_unique_value = user_unique_value\n",
    "        self.item_unique_value = item_unique_value\n",
    "\n",
    "        self.seed = seed\n",
    "\n",
    "\n",
    "class Continual_Data(object):\n",
    "    def make_dataset(cls, train_df, val_df, test_df, seed):\n",
    "        \"\"\"\n",
    "        TODO:: \n",
    "        기존 데이터 중 user-id mapping : 기존 유저아이디를 1부터 해서 점점 증가시킴\n",
    "        기존 데이터 중 item-id mapping : 기존 item아이디를 1부터 해서 점점 증가시킴\n",
    "        consume-set이라고 기존 유저가 소비한 아이템과 아이템을 소비한 유저 매핑 만들기\n",
    "        생성해서 data_info에 집어넣기\n",
    "        최종적으로 pytorch-graph형태의 데이터셋 생성\n",
    "        \"\"\"\n",
    "        total_df = pd.concat([train_df, val_df, test_df])\n",
    "        # 유저 아이디 매핑 \n",
    "        user_set = set(total_df['user'].unique())\n",
    "        item_set = set(total_df['item'].unique())\n",
    "\n",
    "        n_user = len(user_set)\n",
    "        n_item = len(item_set)\n",
    "\n",
    "        user_id_mapping = {user: id for user, id in zip(user_set, range(n_user))}\n",
    "        item_id_mapping = {item: id for item, id in zip(item_set, range(n_item))}\n",
    "        id_user_mapping = {id: user for user, id in user_id_mapping.items()}\n",
    "        id_item_mapping = {id: item for item, id in item_id_mapping.items()}\n",
    "\n",
    "        \"\"\"\n",
    "        TODO:: 만약 train에 없는 유저 혹은 아이템이 val 혹은 test에 등장할경우 제거해주는 코드 작성하기\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "\n",
    "        train_df['user'] = train_df['user'].apply(lambda x: user_id_mapping[x])\n",
    "        train_df['item'] = train_df['item'].apply(lambda x: item_id_mapping[x])\n",
    "        val_df['user'] = val_df['user'].apply(lambda x: user_id_mapping[x])\n",
    "        val_df['item'] = val_df['item'].apply(lambda x: item_id_mapping[x])\n",
    "        test_df['user'] = test_df['user'].apply(lambda x: user_id_mapping[x])\n",
    "        test_df['item'] = test_df['item'].apply(lambda x: item_id_mapping[x])\n",
    "\n",
    "\n",
    "        user_consumed = train_df.groupby('user')['item'].apply(list).reset_index(name=\"consumed_items\")\n",
    "        item_consumed = train_df.groupby('item')['user'].apply(list).reset_index(name=\"consumed_users\")\n",
    "\n",
    "        user_consumed = user_consumed.to_dict('list')\n",
    "        item_consumed = item_consumed.to_dict('list')\n",
    "\n",
    "\n",
    "        # Make pyg data\n",
    "        data = HeteroData()\n",
    "        data['user'].num_nodes = n_user\n",
    "        data['item'].num_nodes = n_item\n",
    "\n",
    "        attr_names = ['edge_train_index', 'edge_val_index', 'edge_test_index', 'edge_total_index']\n",
    "        dataframes = [train_df, val_df, test_df, train_df]\n",
    "        \n",
    "        answer_set = {}\n",
    "\n",
    "        for attr_name, temp_df in zip(attr_names, dataframes):\n",
    "            rows = temp_df['user'].values\n",
    "            cols = temp_df['item'].values\n",
    "            index = torch.tensor([rows, cols])\n",
    "\n",
    "            answer_set[attr_name] = index\n",
    "            \n",
    "            data['user', 'rates', 'item'][attr_name] = index\n",
    "            if attr_name == 'edge_train_index':\n",
    "                data['item', 'rated_by', 'user'][attr_name] = index.flip([0])\n",
    "\n",
    "\n",
    "        return data.to_homogeneous(),answer_set , DataInformation(total_df, user_id_mapping, item_id_mapping, id_user_mapping, id_item_mapping, user_consumed, item_consumed, n_user, n_item, user_set, item_set, seed)\n",
    "\n",
    "        \n",
    "    def merge_dataset(cls, train_df, val_df, test_df ,prev_info: DataInformation):\n",
    "        \"\"\"\n",
    "        TODO::\n",
    "        기존데이터에 id 매핑 뒤쪽에 추가시키고 나머지 데이터 정보도 업데이트 시키기\n",
    "        최종적으로 pytorch-graph형태의 데이터셋 생성\n",
    "        \"\"\"\n",
    "\n",
    "        total_df = pd.concat([train_df, val_df, test_df])\n",
    "\n",
    "        new_user_set = set(total_df['user'].unique())\n",
    "        new_item_set = set(total_df['item'].unique())\n",
    "\n",
    "        total_user_set = prev_info.user_unique_value | new_user_set\n",
    "        total_item_set = prev_info.item_unique_value | new_item_set\n",
    "\n",
    "        new_users = new_user_set.difference(prev_info.user_unique_value)\n",
    "        new_items = new_item_set.difference(prev_info.item_unique_value)\n",
    "\n",
    "        new_n_user = prev_info.n_user + len(new_users)\n",
    "        new_n_item = prev_info.n_item + len(new_items)\n",
    "        \n",
    "\n",
    "        new_user_id_mapping = {user: id for user, id in zip(new_user_set, range(prev_info.n_user+new_n_user))}\n",
    "        new_item_id_mapping = {item: id for item, id in zip(new_item_set, range(prev_info.n_item+new_n_item))}\n",
    "        new_id_user_mapping = {id: user for user, id in new_user_id_mapping.items()}\n",
    "        new_id_item_mapping = {id: item for item, id in new_item_id_mapping.items()}\n",
    "\n",
    "        total_user_id_mapping = prev_info.user_id_mapping\n",
    "        total_item_id_mapping = prev_info.item_id_mapping\n",
    "\n",
    "        total_id_user_mapping = prev_info.id_user_mapping\n",
    "        total_id_item_mapping = prev_info.id_item_mapping\n",
    "\n",
    "        total_user_id_mapping.update(new_user_id_mapping)\n",
    "        total_item_id_mapping.update(new_item_id_mapping)\n",
    "\n",
    "        total_id_user_mapping.update(new_id_user_mapping)\n",
    "        total_id_item_mapping.update(new_id_item_mapping)\n",
    "        \"\"\"\n",
    "        TODO:: 만약 train에 없는 유저 혹은 아이템이 val 혹은 test에 등장할경우 제거해주는 코드 작성하기\n",
    "        \"\"\"\n",
    "\n",
    "        train_df['user'] = train_df['user'].apply(lambda x: total_user_id_mapping[x])\n",
    "        train_df['item'] = train_df['item'].apply(lambda x: total_item_id_mapping[x])\n",
    "        val_df['user'] = val_df['user'].apply(lambda x: total_user_id_mapping[x])\n",
    "        val_df['item'] = val_df['item'].apply(lambda x: total_item_id_mapping[x])\n",
    "        test_df['user'] = test_df['user'].apply(lambda x: total_user_id_mapping[x])\n",
    "        test_df['item'] = test_df['item'].apply(lambda x: total_item_id_mapping[x])\n",
    "\n",
    "        new_user_consumed = train_df.groupby('user')['item'].apply(list).reset_index(name=\"consumed_items\")\n",
    "        new_item_consumed = train_df.groupby('item')['user'].apply(list).reset_index(name=\"consumed_users\")\n",
    "\n",
    "        new_user_consumed = new_user_consumed.to_dict('list')\n",
    "        new_item_consumed = new_item_consumed.to_dict('list')\n",
    "\n",
    "        total_user_consumed = prev_info.user_consumed\n",
    "        total_item_consumed = prev_info.item_consumed\n",
    "\n",
    "        for key, val in new_user_consumed.items():\n",
    "            if key in total_user_consumed.keys():\n",
    "                total_user_consumed[key].extend(val)\n",
    "            else:\n",
    "                total_user_consumed[key] = val\n",
    "        \n",
    "        for key, val in new_item_consumed.items():\n",
    "            if key in total_item_consumed.keys():\n",
    "                total_item_consumed[key].extend(val)\n",
    "            else:\n",
    "                total_item_consumed[key] = val\n",
    "\n",
    "        # Make pyg data\n",
    "        data = HeteroData()\n",
    "        data['user'].num_nodes = new_n_user\n",
    "        data['item'].num_nodes = new_n_item\n",
    "\n",
    "        total_df = pd.concat([prev_info.total_df, train_df])\n",
    "        attr_names = ['edge_train_index', 'edge_val_index', 'edge_test_index', 'edge_total_index']\n",
    "        dataframes = [train_df, val_df, test_df, total_df]\n",
    "\n",
    "        answer_set = {}\n",
    "\n",
    "        for attr_name, temp_df in zip(attr_names, dataframes):\n",
    "            rows = temp_df['user'].values\n",
    "            cols = temp_df['item'].values\n",
    "            index = torch.tensor([rows, cols])\n",
    "            \n",
    "            answer_set[attr_name] = index\n",
    "\n",
    "            data['user', 'rates', 'item'][attr_name] = index\n",
    "\n",
    "            if attr_name == 'edge_train_index':\n",
    "                data['item', 'rated_by', 'user'][attr_name] = index.flip([0])\n",
    "\n",
    "        new_total_df = pd.concat([total_df, val_df, test_df])\n",
    "\n",
    "        return data.to_homogeneous(),answer_set ,DataInformation(new_total_df, total_user_id_mapping, total_item_id_mapping, total_id_user_mapping, total_id_item_mapping, total_user_consumed, total_item_consumed, new_n_user, new_n_item, total_user_set, total_item_set, prev_info.seed)\n",
    "\n",
    "    def save_data_info(self):\n",
    "        \"\"\"\n",
    "        TODO:: \n",
    "        저장해야할 Variable들\n",
    "        user-id mapping\n",
    "        item-id mapping\n",
    "        user-consumeset\n",
    "        item-consumedset\n",
    "        n_user\n",
    "        n_item\n",
    "        user_unique_value\n",
    "        item_unique_value\n",
    "        seed\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric as pyg\n",
    "from torch_geometric.nn import LightGCN\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class ContinualModel(nn.Module):\n",
    "    def __init__(self, data, embedding_dim, device):\n",
    "        super().__init__()\n",
    "        # self.inc = inc\n",
    "        self.data = data\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.device = device\n",
    "        self.model = self.build_model(data)\n",
    "\n",
    "    def forward(self, index, train_edge_label_index):\n",
    "\n",
    "        pos_rank, neg_rank = self.model(index, train_edge_label_index).chunk(2)\n",
    "\n",
    "        loss = self.model.recommendation_loss(\n",
    "            pos_rank,\n",
    "            neg_rank,\n",
    "            node_id=train_edge_label_index.unique(),\n",
    "            lambda_reg = 0.0001\n",
    "        )\n",
    "\n",
    "        # 추가적인 로스는 여기서 계산\n",
    "\n",
    "        return loss , pos_rank\n",
    "\n",
    "    def save_model(self):\n",
    "        \"\"\"\n",
    "        TODO:: embedding 저장하기\n",
    "        \"\"\"\n",
    "\n",
    "    def build_model(self, data):\n",
    "        \n",
    "        return LightGCN(\n",
    "            num_nodes=data.num_nodes,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            num_layers=3\n",
    "        ).to(self.device)\n",
    "    \n",
    "    @torch.no_grad\n",
    "    def rebuild_model(self, emb, prev_num_user, prev_num_item, num_user, num_item):\n",
    "        # 기존 모델에서 임베딩 가져옴 -> 유저와 아이템 기존의 갯수 만큼 나눔\n",
    "        prev_model = 0 # 기존 모델 불러오기\n",
    "        prev_emb = emb\n",
    "        prev_user_emb, prev_item_emb = prev_emb[:prev_num_user], prev_emb[prev_num_user:]\n",
    "\n",
    "        avg_user_emb = torch.mean(prev_user_emb, dim = 0).repeat(num_user-prev_num_user,1)\n",
    "        avg_item_emb = torch.mean(prev_item_emb, dim = 0).repeat(num_item-prev_num_item,1)\n",
    "\n",
    "\n",
    "\n",
    "        new_user_emb = torch.stack([prev_user_emb, avg_user_emb])\n",
    "        new_item_emb = torch.stack([prev_item_emb, avg_item_emb])\n",
    "        \n",
    "        new_total_emb = torch.stack([new_user_emb, new_item_emb])\n",
    "\n",
    "        self.model.embedding.data = new_total_emb.data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "\n",
    "#torch 전에 할댕해줘야함\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())\n",
    "\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.datasets import AmazonBook, MovieLens100K\n",
    "from torch_geometric.nn import LightGCN\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.data import InMemoryDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, data, answer_set, data_info, k: int, batch_size, device):\n",
    "    emb = model.model.get_embedding(data.edge_train_index)\n",
    "    user_emb, book_emb = emb[:data_info.n_user], emb[data_info.n_user:]\n",
    "\n",
    "    precision = recall = total_examples = 0\n",
    "    for start in range(0, data_info.n_user, batch_size):\n",
    "        end = min(start + batch_size, data_info.n_user)\n",
    "        logits = user_emb[start:end] @ book_emb.t()\n",
    "        # # Exclude training edges:\n",
    "        # mask = ((train_edge_label_index[0] >= start) &\n",
    "        #         (train_edge_label_index[0] < end))\n",
    "        # logits[train_edge_label_index[0, mask] - start,\n",
    "        #         train_edge_label_index[1, mask] - data_info.n_user] = float('-inf')\n",
    "        # Exclude total edges:\n",
    "        mask = ((answer_set['edge_total_index'] >= start) &\n",
    "                (answer_set['edge_total_index'] < end))\n",
    "        logits[answer_set['edge_total_index'][mask] - start,\n",
    "                answer_set['edge_total_index'][mask] - data_info.n_user] = float('-inf')\n",
    "\n",
    "        # Computing precision and recall:\n",
    "        ground_truth = torch.zeros_like(logits, dtype=torch.bool)\n",
    "        mask = ((answer_set['edge_val_index'][0] >= start) &\n",
    "                (answer_set['edge_val_index'][0] < end))\n",
    "        ground_truth[answer_set['edge_val_index'][0, mask] - start,\n",
    "                        answer_set['edge_val_index'][1, mask] - data_info.n_user] = True\n",
    "        node_count = degree(answer_set['edge_val_index'][0, mask] - start,\n",
    "                                num_nodes=logits.size(0)).to(device)\n",
    "\n",
    "        topk_index = logits.topk(k, dim=-1).indices\n",
    "        isin_mat = ground_truth.gather(1, topk_index)\n",
    "\n",
    "        precision += float((isin_mat.sum(dim=-1) / k).sum())\n",
    "        recall += float((isin_mat.sum(dim=-1) / node_count.clamp(1e-6)).sum())\n",
    "        total_examples += int((node_count > 0).sum())\n",
    "\n",
    "    return precision / total_examples, recall / total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:14<00:00,  7.13it/s, loss=0.0926, precision=0.0452, recall=0.0213]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [489, 64] at entry 0 and [119, 64] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[254], line 34\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03mTODO::\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m여기서 기존의 모델 가지고 온다음에 rebuild 수행 할 것   \u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 34\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrebuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_emb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_user\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_item\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_user\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_item\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     37\u001b[0m best_recall \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/jiwon/pygbased/.conda/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[251], line 55\u001b[0m, in \u001b[0;36mContinualModel.rebuild_model\u001b[0;34m(self, emb, prev_num_user, prev_num_item, num_user, num_item)\u001b[0m\n\u001b[1;32m     50\u001b[0m avg_user_emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(prev_user_emb, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(num_user\u001b[38;5;241m-\u001b[39mprev_num_user,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     51\u001b[0m avg_item_emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(prev_item_emb, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(num_item\u001b[38;5;241m-\u001b[39mprev_num_item,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m new_user_emb \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprev_user_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavg_user_emb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m new_item_emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([prev_item_emb, avg_item_emb], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     58\u001b[0m new_total_emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([new_user_emb, new_item_emb], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [489, 64] at entry 0 and [119, 64] at entry 1"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data = Continual_Data()\n",
    "\n",
    "best_emb = [0]*len(splitted_ml)\n",
    "info = []\n",
    "for i in range(len(splitted_ml)-1):\n",
    "    if i == 0:\n",
    "        train, val = train_test_split(splitted_ml[i], test_size=0.2)\n",
    "        val, test = train_test_split(val, test_size=0.5)\n",
    "        graph_data, answer_set, data_info = data.make_dataset(train, val, test, 42)\n",
    "        info.append(data_info)\n",
    "    else:\n",
    "        train = splitted_ml[i]\n",
    "        val, test = train_test_split(splitted_ml[i+1], test_size=0.5)\n",
    "        graph_data, answer_set, data_info = data.merge_dataset(train, val, test, data_info)\n",
    "        info.append(data_info)\n",
    "    graph_data.to(device)\n",
    "    batch_size = 1024\n",
    "    mask = graph_data.edge_train_index[0] < graph_data.edge_train_index[1]\n",
    "    train_edge_label_index = graph_data.edge_train_index[:, mask]\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        range(train_edge_label_index.size(1)),\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    model = ContinualModel(graph_data, 64, device)\n",
    "\n",
    "    \"\"\"\n",
    "    TODO::\n",
    "    여기서 기존의 모델 가지고 온다음에 rebuild 수행 할 것   \n",
    "    \"\"\"\n",
    "    if i != 0:\n",
    "        model.rebuild_model(best_emb[i-1], info[i-1].n_user, info[i-1].n_item, data_info.n_user, data_info.n_item)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    best_recall = 0\n",
    "    with tqdm(range(100)) as tepoch:\n",
    "\n",
    "        for epoch in tepoch:\n",
    "            \n",
    "            train_loader = torch.utils.data.DataLoader(\n",
    "            range(train_edge_label_index.size(1)),\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size,\n",
    "            )   \n",
    "            \n",
    "\n",
    "            total_loss = total_examples = 0\n",
    "\n",
    "            for index in train_loader:\n",
    "                # Sample positive and negative labels.\n",
    "                pos_edge_label_index = train_edge_label_index[:, index]\n",
    "                neg_edge_label_index = torch.stack([\n",
    "                    pos_edge_label_index[0],\n",
    "                    torch.randint(data_info.n_user, data_info.n_user + data_info.n_item,\n",
    "                                (index.numel(), ), device=device)\n",
    "                ], dim=0)\n",
    "                edge_label_index = torch.cat([\n",
    "                    pos_edge_label_index,\n",
    "                    neg_edge_label_index,\n",
    "                ], dim=1)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss, pos_rank = model(graph_data.edge_train_index, edge_label_index)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += float(loss) * pos_rank.numel()\n",
    "                total_examples += pos_rank.numel()\n",
    "            recall, precision = evaluate(model, graph_data, answer_set, data_info,20, batch_size, device)\n",
    "            if recall > best_recall:\n",
    "                best_recall = recall\n",
    "                best_emb[i] = model.model.embedding.weight.detach()\n",
    "            tepoch.set_postfix(loss=total_loss / total_examples, recall = recall, precision = precision)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>258</td>\n",
       "      <td>254</td>\n",
       "      <td>4</td>\n",
       "      <td>874724710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>258</td>\n",
       "      <td>285</td>\n",
       "      <td>4</td>\n",
       "      <td>874724727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>258</td>\n",
       "      <td>297</td>\n",
       "      <td>4</td>\n",
       "      <td>874724754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>258</td>\n",
       "      <td>184</td>\n",
       "      <td>4</td>\n",
       "      <td>874724781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>258</td>\n",
       "      <td>172</td>\n",
       "      <td>4</td>\n",
       "      <td>874724843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48971</th>\n",
       "      <td>767</td>\n",
       "      <td>1013</td>\n",
       "      <td>2</td>\n",
       "      <td>882816126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48972</th>\n",
       "      <td>828</td>\n",
       "      <td>249</td>\n",
       "      <td>3</td>\n",
       "      <td>882816754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48973</th>\n",
       "      <td>828</td>\n",
       "      <td>221</td>\n",
       "      <td>4</td>\n",
       "      <td>882816987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48974</th>\n",
       "      <td>667</td>\n",
       "      <td>895</td>\n",
       "      <td>4</td>\n",
       "      <td>882818549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48975</th>\n",
       "      <td>667</td>\n",
       "      <td>287</td>\n",
       "      <td>4</td>\n",
       "      <td>882818604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48976 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user  item  rating       time\n",
       "0       258   254       4  874724710\n",
       "1       258   285       4  874724727\n",
       "2       258   297       4  874724754\n",
       "3       258   184       4  874724781\n",
       "4       258   172       4  874724843\n",
       "...     ...   ...     ...        ...\n",
       "48971   767  1013       2  882816126\n",
       "48972   828   249       3  882816754\n",
       "48973   828   221       4  882816987\n",
       "48974   667   895       4  882818549\n",
       "48975   667   287       4  882818604\n",
       "\n",
       "[48976 rows x 4 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_ml[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
